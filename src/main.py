"""Main.

This module implements a chatbot using Gradio and LangChain for question-answering based on document retrieval
and language model generation.
"""

import logging
from typing import Any

import gradio as gr
from gradio_pdf import PDF
from langchain import hub
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langgraph.graph import START, StateGraph
from typing_extensions import TypedDict

from config import ChatbotSettings
from create_vectorstore import vectorstore

config = ChatbotSettings()


def initialize_llm(model_name: str) -> ChatOpenAI:
    """Initialize OpenAI chat model."""
    return ChatOpenAI(model=model_name)


llm = initialize_llm(config.openai_model)


def load_prompt(prompt_name: str) -> Any:  # noqa: ANN401
    """Load pre-defined prompt for question-answering from LangChain."""
    return hub.pull(prompt_name)


prompt = load_prompt(config.prompt_name)


class State(TypedDict):
    """A TypedDict representing the state of a chatbot interaction.

    Attributes:
        question (str): The question asked by the user.
        num_references (int): The number of reference documents to use as context for the question.
        context (list[Document]): A list of Document objects providing context for the question.
        answer (str): The answer generated by the chatbot.
    """

    question: str
    num_references: int
    context: list[Document]
    answer: str


def retrieve_documents(state: State) -> dict[str, list[Document]]:
    """Retrieve relevant documents based on the question."""
    try:
        retrieved_docs = vectorstore.similarity_search(state["question"], state["num_references"])
    except Exception:
        logging.exception("An error occurred while retrieving documents.")
        return {"context": []}
    else:
        return {"context": retrieved_docs}


def generate_answer(state: State) -> dict[str, str]:
    """Generate an answer based on the retrieved documents and question."""
    try:
        docs_content = "\n\n".join(doc.page_content for doc in state["context"])
        messages = prompt.invoke({"question": state["question"], "context": docs_content})
        response = llm.invoke(messages)
    except Exception:
        msg = "An error occurred while generating the answer."
        logging.exception(msg)
        return {"answer": msg}
    else:
        return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([retrieve_documents, generate_answer])
graph_builder.add_edge(START, "retrieve_documents")
graph = graph_builder.compile()


def generate_response(
    question: str,
    history: list[dict[str, str]],  # noqa: ARG001
    num_references: int,
) -> tuple[Any, *tuple[PDF, ...]]:
    """Generate an answer for the given question using the state graph."""
    response: dict[str, Any] = graph.invoke({"question": question, "num_references": num_references})
    sources = [
        PDF(source_info.metadata["source"], starting_page=source_info.metadata["page"] + 1)
        for source_info in response["context"]
    ]
    sources += [PDF() for _ in range(config.max_num_context_sources - num_references)]
    return response["answer"], *sources


if __name__ == "__main__":
    with gr.Blocks() as demo:
        file_outputs = [PDF(render=False) for _ in range(config.max_num_context_sources)]

        gr.ChatInterface(
            fn=generate_response,
            type="messages",
            additional_inputs=gr.Slider(
                minimum=1,
                maximum=config.max_num_context_sources,
                value=3,
                step=1,
                label="Number of references",
            ),
            additional_outputs=file_outputs,
        )

        for i in range(config.max_num_context_sources):
            with gr.Accordion(label=f"Source {i + 1}", open=False):
                file_outputs[i].render()

    demo.launch(share=True)
